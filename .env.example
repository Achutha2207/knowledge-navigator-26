# Backend Configuration

# Ollama Configuration
# Base URL where Ollama is running (default: http://localhost:11434)
OLLAMA_BASE_URL=http://localhost:11434

# Model name to use for inference (e.g., llama3.1, mistral, codellama)
OLLAMA_MODEL=llama3.1

# Environment setting (development, staging, production)
ENVIRONMENT=development

# Frontend Configuration (for Vite)

# Backend API URL for the frontend to call
# In development: http://localhost:8000
# In production: leave empty or set to your production backend URL
VITE_API_BASE_URL=http://localhost:8000

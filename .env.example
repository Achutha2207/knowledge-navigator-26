# Ollama Configuration
# Base URL where Ollama is running (default: http://localhost:11434)
OLLAMA_BASE_URL=http://localhost:11434

# Model name to use for inference (e.g., mistral, llama2, codellama)
MODEL_NAME=mistral

# Environment setting (development, staging, production)
ENVIRONMENT=development
